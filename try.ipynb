{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 PDF files to process...\n",
      "==================================================\n",
      "[1/16] Processing: Lia_1.pdf - ✓ Extracted 6 images\n",
      "[2/16] Processing: Lia_2.pdf - ✓ Extracted 4 images\n",
      "[3/16] Processing: Lia_3.pdf - ✓ Extracted 7 images\n",
      "[4/16] Processing: Lia_4.pdf - ✓ Extracted 4 images\n",
      "[5/16] Processing: Lia_5.pdf - ✓ Extracted 5 images\n",
      "[6/16] Processing: Lia_6.pdf - ✓ Extracted 3 images\n",
      "[7/16] Processing: Lia_7.pdf - ✓ Extracted 4 images\n",
      "[8/16] Processing: Lia_8.pdf - ✓ Extracted 6 images\n",
      "[9/16] Processing: Lib_1.pdf - ✓ Extracted 14 images\n",
      "[10/16] Processing: Lib_2.pdf - ✓ Extracted 24 images\n",
      "[11/16] Processing: Lib_3.pdf - ✓ Extracted 13 images\n",
      "[12/16] Processing: Lib_4.pdf - ✓ Extracted 7 images\n",
      "[13/16] Processing: Lib_5.pdf - ✓ Extracted 9 images\n",
      "[14/16] Processing: Lib_6.pdf - ✓ Extracted 12 images\n",
      "[15/16] Processing: Lib_7.pdf - ✓ Extracted 9 images\n",
      "[16/16] Processing: Lib_8.pdf - ✓ Extracted 7 images\n",
      "==================================================\n",
      "Processing completed! 134 images extracted\n",
      "Results saved to: extracted_images_report.csv\n",
      "Images saved to: c:\\Users\\dkbay\\Downloads\\code\\Project with Thinh\\LLM\\Paper_retrieval\\extracted_images\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import spacy\n",
    "\n",
    "\n",
    "def get_configuration_constants():\n",
    "    return {\n",
    "        'DEFAULT_KEYWORDS': [\n",
    "            \"Cu||Li\", \"Li||Cu\", \"Li-Cu\", \"Cu-Li\", 'Aurbach',\n",
    "            \"coulombic efficiency\", \"CE\", \"coulombic efficiencies\",\n",
    "            \"conductivity\", \"conductivities\", \n",
    "            \"viscosity\", \"viscosities\"\n",
    "        ],\n",
    "        'SIMILARITY_THRESHOLD': 0.5,\n",
    "        'SPACY_MODEL': \"en_core_web_md\"\n",
    "    }\n",
    "\n",
    "\n",
    "def load_spacy_model(model_name=None):\n",
    "    if model_name is None:\n",
    "        model_name = get_configuration_constants()['SPACY_MODEL']\n",
    "    \n",
    "    try:\n",
    "        return spacy.load(model_name)\n",
    "    except OSError:\n",
    "        print(f\"Model '{model_name}' not found. Trying 'en_core_web_sm'...\")\n",
    "        try:\n",
    "            return spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            raise RuntimeError(\"No spaCy model found. Please install with: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "\n",
    "def prepare_keyword_embeddings(nlp_model, keywords):\n",
    "    keyword_embeddings = {}\n",
    "    for kw in keywords:\n",
    "        doc = nlp_model(kw)\n",
    "        if doc.vector_norm:\n",
    "            keyword_embeddings[kw] = doc.vector / doc.vector_norm\n",
    "    return keyword_embeddings\n",
    "\n",
    "\n",
    "def calculate_similarity(nlp_model, caption, keyword_embeddings, similarity_threshold):\n",
    "    caption_doc = nlp_model(caption)\n",
    "    if not caption_doc.vector_norm:\n",
    "        return []\n",
    "    \n",
    "    caption_vec = caption_doc.vector / caption_doc.vector_norm\n",
    "    found_keywords = []\n",
    "    \n",
    "    for kw, kw_vec in keyword_embeddings.items():\n",
    "        similarity = np.dot(caption_vec, kw_vec)\n",
    "        if similarity >= similarity_threshold:\n",
    "            found_keywords.append((kw, similarity))\n",
    "    \n",
    "    found_keywords.sort(key=lambda x: x[1], reverse=True)\n",
    "    return found_keywords\n",
    "\n",
    "\n",
    "def is_caption_text(text):\n",
    "    # Check if caption starts with specific keywords\n",
    "    if re.search(r\"^(Figure|Fig\\.?|Supplementary Figure)\\b\", text.strip(), re.IGNORECASE):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def find_image_caption(page, img_info, text_blocks):\n",
    "    img_rect = page.get_image_bbox(img_info)\n",
    "    best_caption = \"\"\n",
    "    best_distance = float('inf')\n",
    "    \n",
    "    for block in text_blocks:\n",
    "        if \"lines\" not in block:\n",
    "            continue\n",
    "            \n",
    "        block_rect = fitz.Rect(block[\"bbox\"])\n",
    "        block_text = \" \".join([span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]]).strip()\n",
    "        \n",
    "        if not is_caption_text(block_text):\n",
    "            continue\n",
    "        \n",
    "        vertical_distance = abs(block_rect.y0 - img_rect.y1)\n",
    "        horizontal_overlap = min(img_rect.x1, block_rect.x1) - max(img_rect.x0, block_rect.x0)\n",
    "        \n",
    "        if horizontal_overlap > 0 and vertical_distance < 100:\n",
    "            if vertical_distance < best_distance:\n",
    "                best_caption = block_text\n",
    "                best_distance = vertical_distance\n",
    "    \n",
    "    return best_caption\n",
    "\n",
    "\n",
    "def extract_and_save_image(pdf, xref, page_num, output_dir, pdf_name, img_index):\n",
    "    try:\n",
    "        base_image = pdf.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]\n",
    "        image_ext = base_image[\"ext\"]\n",
    "    except Exception as e:\n",
    "        print(f\"    - Could not extract image on page {page_num+1}: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    image_filename = f\"{pdf_name}_p{page_num+1}_fig{img_index+1}.{image_ext}\"\n",
    "    image_path = os.path.join(output_dir, image_filename)\n",
    "    \n",
    "    with open(image_path, \"wb\") as img_file:\n",
    "        img_file.write(image_bytes)\n",
    "    \n",
    "    return image_filename, image_path\n",
    "\n",
    "\n",
    "def process_single_pdf(pdf_path, nlp_model, keyword_embeddings, similarity_threshold, output_dir):\n",
    "    pdf = fitz.open(pdf_path)\n",
    "    results = []\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    \n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf.load_page(page_num)\n",
    "        \n",
    "        images = page.get_images(full=True)\n",
    "        if not images:\n",
    "            continue\n",
    "        \n",
    "        text_blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        \n",
    "        for img_index, img_info in enumerate(images):\n",
    "            xref = img_info[0]\n",
    "            \n",
    "            caption = find_image_caption(page, img_info, text_blocks)\n",
    "            \n",
    "            if caption:\n",
    "                found_keywords = calculate_similarity(nlp_model, caption, keyword_embeddings, similarity_threshold)\n",
    "                \n",
    "                if found_keywords:\n",
    "                    image_result = extract_and_save_image(pdf, xref, page_num, output_dir, pdf_name, img_index)\n",
    "                    if image_result:\n",
    "                        image_filename, image_path = image_result\n",
    "                        results.append({\n",
    "                            'image_filename': image_filename,\n",
    "                            'image_path': image_path,\n",
    "                            'page': page_num + 1,\n",
    "                            'caption': caption,\n",
    "                            'keywords': found_keywords\n",
    "                        })\n",
    "    \n",
    "    pdf.close()\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_all_pdf_files(paper_folder, si_folder):\n",
    "    \"\"\"Get all PDF files from both folders\"\"\"\n",
    "    all_files = []\n",
    "    \n",
    "    # Get files from paper folder\n",
    "    if os.path.exists(paper_folder):\n",
    "        paper_files = [f for f in os.listdir(paper_folder) if f.lower().endswith('.pdf')]\n",
    "        for f in paper_files:\n",
    "            all_files.append((os.path.join(paper_folder, f), f))\n",
    "    \n",
    "    # Get files from SI folder\n",
    "    if os.path.exists(si_folder):\n",
    "        si_files = [f for f in os.listdir(si_folder) if f.lower().endswith('.pdf')]\n",
    "        for f in si_files:\n",
    "            all_files.append((os.path.join(si_folder, f), f))\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "\n",
    "def write_results_to_csv(output_csv, all_results):\n",
    "    fieldnames = ['pdf_file', 'image_file', 'page', 'caption', 'keywords', 'similarity_scores']\n",
    "    \n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for result in all_results:\n",
    "            writer.writerow({\n",
    "                'pdf_file': result['pdf_file'],\n",
    "                'image_file': result['image_filename'],\n",
    "                'page': result['page'],\n",
    "                'caption': result['caption'],\n",
    "                'keywords': \"; \".join([k for k, _ in result['keywords']]),\n",
    "                'similarity_scores': \"; \".join([f\"{s:.3f}\" for _, s in result['keywords']])\n",
    "            })\n",
    "\n",
    "\n",
    "def process_pdf_folders(paper_folder, si_folder, output_csv, extracted_images_folder, keywords=None, similarity_threshold=None):\n",
    "    config = get_configuration_constants()\n",
    "    \n",
    "    if keywords is None:\n",
    "        keywords = config['DEFAULT_KEYWORDS']\n",
    "    if similarity_threshold is None:\n",
    "        similarity_threshold = config['SIMILARITY_THRESHOLD']\n",
    "    \n",
    "    nlp = load_spacy_model()\n",
    "    keyword_embeddings = prepare_keyword_embeddings(nlp, keywords)\n",
    "    \n",
    "    output_dir = os.path.join(os.getcwd(), extracted_images_folder)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Get all PDF files from both folders\n",
    "    all_files = get_all_pdf_files(paper_folder, si_folder)\n",
    "    total_files = len(all_files)\n",
    "    \n",
    "    print(f\"Found {total_files} PDF files to process...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for idx, (file_path, filename) in enumerate(all_files, 1):\n",
    "        print(f\"[{idx}/{total_files}] Processing: {filename}\", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            pdf_results = process_single_pdf(file_path, nlp, keyword_embeddings, similarity_threshold, output_dir)\n",
    "            \n",
    "            for result in pdf_results:\n",
    "                result['pdf_file'] = filename\n",
    "                all_results.append(result)\n",
    "            \n",
    "            if pdf_results:\n",
    "                print(f\" - ✓ Extracted {len(pdf_results)} images\")\n",
    "            else:\n",
    "                print(f\" - No images found\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\" - ✗ Error: {str(e)}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    write_results_to_csv(output_csv, all_results)\n",
    "    print(f\"Processing completed! {len(all_results)} images extracted\")\n",
    "    print(f\"Results saved to: {output_csv}\")\n",
    "    print(f\"Images saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    paper_folder = os.path.join(os.getcwd(), \"paper\")\n",
    "    si_folder = os.path.join(os.getcwd(), \"paper_SI\")\n",
    "    OUTPUT_CSV = \"extracted_images_report.csv\"\n",
    "    EXTRACTED_IMAGES_FOLDER = \"extracted_images\"\n",
    "    \n",
    "    config = get_configuration_constants()\n",
    "    \n",
    "    process_pdf_folders(\n",
    "        paper_folder=paper_folder,\n",
    "        si_folder=si_folder,\n",
    "        output_csv=OUTPUT_CSV,\n",
    "        extracted_images_folder=EXTRACTED_IMAGES_FOLDER,\n",
    "        keywords=config['DEFAULT_KEYWORDS'],\n",
    "        similarity_threshold=config['SIMILARITY_THRESHOLD']\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
